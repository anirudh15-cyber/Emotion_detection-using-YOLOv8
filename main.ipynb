{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from fastapi import FastAPI, UploadFile, File, BackgroundTasks\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from fastapi.responses import FileResponse,HTMLResponse, JSONResponse, StreamingResponse\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from starlette.requests import Request\n",
    "import logging\n",
    "import requests\n",
    "import random\n",
    "import shutil\n",
    "import nest_asyncio\n",
    "import uvicorn\n",
    "import asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the valid data into training and val of Widerface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Source dirs \n",
    "source_images_dir = \"D:/Project/Emotion_detection-using-YOLOv8/WIDER-FACE-1/test/images\"\n",
    "source_labels_dir = \"D:/Project/Emotion_detection-using-YOLOv8/WIDER-FACE-1/test/labels\"\n",
    "\n",
    "# Target dirs\n",
    "train_images_dir = \"D:/Project/Emotion_detection-using-YOLOv8/WIDER-FACE-1/train/images\"\n",
    "train_labels_dir = \"D:/Project/Emotion_detection-using-YOLOv8/WIDER-FACE-1/train/labels\"\n",
    "valid_images_dir = \"D:/Project/Emotion_detection-using-YOLOv8/WIDER-FACE-1/val/images\"\n",
    "valid_labels_dir = \"D:/Project/Emotion_detection-using-YOLOv8/WIDER-FACE-1/val/labels\"\n",
    "\n",
    "# Create directories\n",
    "for d in [train_images_dir, train_labels_dir, valid_images_dir, valid_labels_dir]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# Get all images\n",
    "all_images = [f for f in os.listdir(source_images_dir) if f.endswith(\".jpg\")]\n",
    "\n",
    "# Shuffle and split\n",
    "random.shuffle(all_images)\n",
    "train_images = all_images[:int(0.8 * len(all_images))]\n",
    "val_images = all_images[int(0.8 * len(all_images)):]\n",
    "\n",
    "# Move training set\n",
    "for img in train_images:\n",
    "    shutil.move(os.path.join(source_images_dir, img), os.path.join(train_images_dir, img))\n",
    "    label_file = img.replace(\".jpg\", \".txt\")\n",
    "    if os.path.exists(os.path.join(source_labels_dir, label_file)):\n",
    "        shutil.move(os.path.join(source_labels_dir, label_file), os.path.join(train_labels_dir, label_file))\n",
    "\n",
    "# Move validation set\n",
    "for img in val_images:\n",
    "    shutil.move(os.path.join(source_images_dir, img), os.path.join(valid_images_dir, img))\n",
    "    label_file = img.replace(\".jpg\", \".txt\")\n",
    "    if os.path.exists(os.path.join(source_labels_dir, label_file)):\n",
    "        shutil.move(os.path.join(source_labels_dir, label_file), os.path.join(valid_labels_dir, label_file))\n",
    "\n",
    "print(f\"Train set has {len(train_images)} images, and validation set has {len(val_images)} images.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine Tuning Yolo with WIDER-Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall torch torchvision torchaudio -y\n",
    "!pip cache purge\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the pre-trained YOLO model\n",
    "yolo = YOLO('yolov8n.pt')\n",
    "\n",
    "yolo.train(\n",
    "    data='WIDER-FACE-1/data.yaml',\n",
    "    epochs=20,                  \n",
    "    batch=8,                     \n",
    "    imgsz=416,                       \n",
    "    project='runs/train',          \n",
    "    name='best',                 \n",
    "    save=True,                   \n",
    "    device='cuda',                  \n",
    "    lr0=0.01,                      \n",
    "    lrf=0.1,                        \n",
    "    conf=0.001,                    \n",
    "    half=True                       \n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo = YOLO('yolov8n.pt')\n",
    "# After training, evaluate the model\n",
    "valid_results = yolo.val()\n",
    "\n",
    "# Print evaluation results \n",
    "print(valid_results) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating Results and post-processing using Non maxima Suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the trained YOLO model\n",
    "yolo = YOLO('best.pt')  \n",
    "\n",
    "\n",
    "results = yolo.predict(\n",
    "    source='uploads/',  \n",
    "    conf=0.5,              \n",
    "    iou=0.4,               \n",
    "    save=True,           \n",
    "    device='cuda'           \n",
    ")\n",
    "\n",
    "# Print detailed results\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"Files in uploads/:\", os.listdir(\"uploads\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING USING FER 2013 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize image data generator with rescaling and augmentation\n",
    "train_data_gen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "validation_data_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Preprocess all train images\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "    'data/train',\n",
    "    target_size=(48, 48),\n",
    "    batch_size=64,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Preprocess all test images\n",
    "validation_generator = validation_data_gen.flow_from_directory(\n",
    "    'data/test',\n",
    "    target_size=(48, 48),\n",
    "    batch_size=64,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Create model structure\n",
    "emotion_model = Sequential()\n",
    "\n",
    "# Specify input shape using Input layer\n",
    "emotion_model.add(Input(shape=(48, 48, 1))) \n",
    "\n",
    "# Convolutional layers with Batch Normalization and filter count\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(BatchNormalization())\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(BatchNormalization())\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.3))\n",
    "\n",
    "emotion_model.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(BatchNormalization())\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(256, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(BatchNormalization())\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.3))\n",
    "\n",
    "emotion_model.add(Flatten())\n",
    "\n",
    "# Fully connected layers\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.4))\n",
    "emotion_model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "# Apply learning rate decay with ExponentialDecay\n",
    "lr_schedule = ExponentialDecay(initial_learning_rate=0.001, \n",
    "                               decay_steps=100000, \n",
    "                               decay_rate=0.96, \n",
    "                               staircase=True)\n",
    "\n",
    "# Compile the model\n",
    "emotion_model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=Adam(learning_rate=lr_schedule),\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the neural network/model with early stopping and learning rate scheduling\n",
    "emotion_model_info = emotion_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=28709 // 64,\n",
    "    epochs=100,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=7178 // 64,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Save model structure in JSON file\n",
    "model_json = emotion_model.to_json()\n",
    "with open(\"emotion_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Save trained model weights in .h5 file\n",
    "emotion_model.save_weights('emotion_model.weights.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing and evaluating Matrix using Face++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(title=\"Emotion Detector\")\n",
    "\n",
    "# Enable CORS\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  \n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "# Face++ API credentials\n",
    "API_KEY = \"mi11HLqpDSpqFj8lWz6eC82EwHlPICOh\"\n",
    "API_SECRET = \"l7s7lwVVkO7CDN5oS3s5uT27tjlNlE40\"\n",
    "FACE_PLUS_PLUS_URL = \"https://api-us.faceplusplus.com/facepp/v3/detect\"\n",
    "\n",
    "UPLOAD_DIR = 'uploads'\n",
    "TEMP_DIR = 'temp'\n",
    "OUTPUT_DIR = 'output'\n",
    "os.makedirs(UPLOAD_DIR, exist_ok=True)\n",
    "os.makedirs(TEMP_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Serve uploaded files from the 'uploads' directory\n",
    "app.mount(\"/uploads\", StaticFiles(directory=UPLOAD_DIR), name=\"uploads\")\n",
    "\n",
    "\n",
    "\n",
    "# Load YOLOv8 face detection model\n",
    "face_model = YOLO('best.pt')  # Replace with your YOLOv8 model path\n",
    "\n",
    "# Load emotion classification model\n",
    "with open('emotion_model.json', 'r') as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "emotion_model = model_from_json(loaded_model_json)\n",
    "emotion_model.load_weights(\"emotion_model.weights.h5\")\n",
    "print(\"Loaded emotion model from disk\")\n",
    "\n",
    "# Emotion labels\n",
    "emotion_dict = {\n",
    "    0: \"Angry\",\n",
    "    1: \"Disgust\",\n",
    "    2: \"Fear\",\n",
    "    3: \"Happy\",\n",
    "    4: \"Neutral\",\n",
    "    5: \"Sad\",\n",
    "    6: \"Surprise\"\n",
    "}\n",
    "\n",
    "def map_predicted_label(predicted_emotion):\n",
    "    if predicted_emotion == \"Sad\":\n",
    "        return \"sadness\"\n",
    "    elif predicted_emotion == \"Neutral\":\n",
    "        return \"neutral\"\n",
    "\n",
    "    return predicted_emotion\n",
    "\n",
    "# Function to detect emotion using Face++ API\n",
    "def detect_emotion(image_path):\n",
    "    with open(image_path, 'rb') as f:\n",
    "        img_data = f.read()\n",
    "    data = {\n",
    "        'api_key': API_KEY,\n",
    "        'api_secret': API_SECRET,\n",
    "        'return_attributes': 'emotion'\n",
    "    }\n",
    "    files = {'image_file': img_data}\n",
    "    response = requests.post(FACE_PLUS_PLUS_URL, data=data, files=files)\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        if 'faces' in result:\n",
    "            emotions = result['faces'][0]['attributes']['emotion']\n",
    "            predicted_emotion = max(emotions, key=emotions.get)\n",
    "            return predicted_emotion\n",
    "        else:\n",
    "            logging.warning(\"No faces detected\")\n",
    "            return None\n",
    "    else:\n",
    "        logging.error(f\"Error in API request: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Function to detect faces using YOLO\n",
    "def detect_faces(frame, face_model):\n",
    "    results = face_model(frame)\n",
    "    boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "    confidences = results[0].boxes.conf.cpu().numpy()\n",
    "    return boxes, confidences\n",
    "\n",
    "def classify_emotion(face_img, emotion_model):\n",
    "    gray_face = cv2.cvtColor(face_img, cv2.COLOR_BGR2GRAY)\n",
    "    resized_face = cv2.resize(gray_face, (48, 48))\n",
    "    processed_face = np.expand_dims(resized_face, axis=-1) \n",
    "    processed_face = np.expand_dims(processed_face, axis=0)  \n",
    "    processed_face = processed_face / 255.0  \n",
    "    prediction = emotion_model.predict(processed_face)\n",
    "    return map_predicted_label(emotion_dict[np.argmax(prediction)])\n",
    "\n",
    "# Function to process video frames and assign emotions\n",
    "\n",
    "def process_video_for_emotions(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    ground_truth = {}\n",
    "    predictions = {}\n",
    "    frame_interval = 5  \n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_count += 1\n",
    "        if frame_count % frame_interval != 0:\n",
    "            continue\n",
    "        temp_frame_path = os.path.join(TEMP_DIR, f\"frame_{frame_count}.jpg\")\n",
    "        cv2.imwrite(temp_frame_path, frame)\n",
    "        \n",
    "        predicted_emotion = detect_emotion(temp_frame_path)\n",
    "        if predicted_emotion:\n",
    "            ground_truth[frame_count] = [predicted_emotion]\n",
    "        \n",
    "        boxes, confidences = detect_faces(frame, face_model)\n",
    "        for box, confidence in zip(boxes, confidences):\n",
    "            if confidence > 0.7:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                face_roi = frame[y1:y2, x1:x2]\n",
    "\n",
    "                predicted_emotion_model = classify_emotion(face_roi, emotion_model)\n",
    "                font_scale = 1\n",
    "                thickness = 2\n",
    "                (text_width, text_height), baseline = cv2.getTextSize(predicted_emotion, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)\n",
    "\n",
    "                text_x = x1\n",
    "                text_y = y1 - 10\n",
    "                if text_x + text_width > frame.shape[1]:\n",
    "                    text_x = frame.shape[1] - text_width - 10\n",
    "                if text_y - text_height < 0:\n",
    "                    text_y = y1 + 10\n",
    "\n",
    "                # Display the emotion label above the detected face\n",
    "                cv2.putText(frame, predicted_emotion, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 255, 0), thickness)\n",
    "                predictions[frame_count] = [predicted_emotion_model]\n",
    "        \n",
    "        os.remove(temp_frame_path)\n",
    "        cv2.imshow('Emotion Recognition with YOLO', frame)\n",
    "\n",
    "        # Exit on pressing 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "\n",
    "    return ground_truth, predictions\n",
    "\n",
    "\n",
    "\n",
    "# Function to evaluate model\n",
    "def evaluate_model(ground_truth, predictions):\n",
    "    flat_predictions = [label for frame_preds in predictions.values() for label in frame_preds]\n",
    "    flat_true_labels = [label for frame_labels in ground_truth.values() for label in frame_labels]\n",
    "    \n",
    "    min_length = min(len(flat_true_labels), len(flat_predictions))\n",
    "    flat_true_labels = flat_true_labels[:min_length]\n",
    "    flat_predictions = flat_predictions[:min_length]\n",
    "\n",
    "    precision = precision_score(flat_true_labels, flat_predictions, average='macro')\n",
    "    recall = recall_score(flat_true_labels, flat_predictions, average='macro')\n",
    "    f1 = f1_score(flat_true_labels, flat_predictions, average='macro')\n",
    "\n",
    "    logging.info(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-score: {f1:.2f}\")\n",
    "\n",
    "\n",
    "@app.post(\"/process_video\")\n",
    "async def process_video(file: UploadFile = File(...)):\n",
    "    file_path = os.path.join(UPLOAD_DIR, file.filename)\n",
    "    with open(file_path, \"wb\") as buffer:\n",
    "        shutil.copyfileobj(file.file, buffer)\n",
    "    ground_truth, predictions = process_video_for_emotions(file_path)\n",
    "    evaluate_model(ground_truth, predictions)\n",
    "    return {\"filename\": file.filename, \"message\": \"Processing complete\"}\n",
    "\n",
    "@app.get(\"/uploads/{filename}\")\n",
    "async def stream_full_video(filename: str):\n",
    "    video_path = os.path.join(UPLOAD_DIR, filename)\n",
    "    if not os.path.exists(video_path):\n",
    "        return {\"error\": \"Video not found\"}\n",
    "    return FileResponse(video_path, media_type=\"video/mp4\")\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def get_html():\n",
    "    return HTMLResponse(content=open(\"index.html\").read(), status_code=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrating YOLOv8 with Emotion classification and deploying it to Web using FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(title=\"Emotion Detector\")\n",
    "\n",
    "# Enable CORS\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"], \n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Directory setup\n",
    "UPLOAD_DIR = 'uploads'\n",
    "os.makedirs(UPLOAD_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# Serve uploaded files from the 'uploads' directory\n",
    "app.mount(\"/uploads\", StaticFiles(directory=UPLOAD_DIR), name=\"uploads\")\n",
    "\n",
    "# Face++ API credentials\n",
    "API_KEY = \"mi11HLqpDSpqFj8lWz6eC82EwHlPICOh\"\n",
    "API_SECRET = \"l7s7lwVVkO7CDN5oS3s5uT27tjlNlE40\"\n",
    "FACE_PLUS_PLUS_URL = \"https://api-us.faceplusplus.com/facepp/v3/detect\"\n",
    "\n",
    "# Load YOLOv8 face detection model\n",
    "face_model = YOLO('best.pt')  # Replace with your YOLOv8 model path\n",
    "\n",
    "# Load emotion classification model\n",
    "with open('emotion_model.json', 'r') as json_file:\n",
    "    loaded_model_json = json_file.read()\n",
    "emotion_model = model_from_json(loaded_model_json)\n",
    "emotion_model.load_weights(\"emotion_model.weights.h5\")\n",
    "print(\"Loaded emotion model from disk\")\n",
    "\n",
    "# Emotion labels\n",
    "emotion_dict = {\n",
    "    0: \"Angry\",\n",
    "    1: \"Disgust\",\n",
    "    2: \"Fear\",\n",
    "    3: \"Happy\",\n",
    "    4: \"Neutral\",\n",
    "    5: \"Sad\",\n",
    "    6: \"Surprise\"\n",
    "}\n",
    "\n",
    "# Function to detect emotion using Face++ API\n",
    "def detect_emotion(image_path):\n",
    "    with open(image_path, 'rb') as f:\n",
    "        img_data = f.read()\n",
    "    data = {\n",
    "        'api_key': API_KEY,\n",
    "        'api_secret': API_SECRET,\n",
    "        'return_attributes': 'emotion'\n",
    "    }\n",
    "    files = {'image_file': img_data}\n",
    "    response = requests.post(FACE_PLUS_PLUS_URL, data=data, files=files)\n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        if 'faces' in result:\n",
    "            emotions = result['faces'][0]['attributes']['emotion']\n",
    "            predicted_emotion = max(emotions, key=emotions.get)\n",
    "            return predicted_emotion\n",
    "        else:\n",
    "            logging.warning(\"No faces detected\")\n",
    "            return None\n",
    "    else:\n",
    "        logging.error(f\"Error in API request: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Function to classify emotion using a local model\n",
    "def classify_emotion(face_img, emotion_model):\n",
    "    gray_face = cv2.cvtColor(face_img, cv2.COLOR_BGR2GRAY)\n",
    "    resized_face = cv2.resize(gray_face, (48, 48))\n",
    "    processed_face = np.expand_dims(resized_face, axis=-1)  # Add channel dimension\n",
    "    processed_face = np.expand_dims(processed_face, axis=0)  # Add batch dimension\n",
    "    processed_face = processed_face / 255.0  # Normalize pixel values\n",
    "    prediction = emotion_model.predict(processed_face)\n",
    "    return emotion_dict[np.argmax(prediction)]\n",
    "\n",
    "# Function to detect faces using YOLO\n",
    "def detect_faces(frame, face_model):\n",
    "    results = face_model(frame)\n",
    "    boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "    confidences = results[0].boxes.conf.cpu().numpy()\n",
    "    return boxes, confidences\n",
    "\n",
    "# Function to detect and classify emotions in video\n",
    "def detect_and_classify_emotions(video_path, output_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    frame_count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, (640, 480))\n",
    "        frame_count += 1\n",
    "\n",
    "        boxes, confidences = detect_faces(frame, face_model)\n",
    "        for box, confidence in zip(boxes, confidences):\n",
    "            if confidence > 0.6:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                face_roi = frame[y1:y2, x1:x2]\n",
    "                if face_roi.size == 0:\n",
    "                    continue\n",
    "                predicted_emotion = classify_emotion(face_roi, emotion_model)\n",
    "\n",
    "                # Draw label inside frame\n",
    "                cv2.putText(\n",
    "                    frame, predicted_emotion, (x1, max(20, y1 - 10)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2\n",
    "                )\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    logging.info(f\"Video processing complete. Processed frames: {frame_count}\")\n",
    "\n",
    "\n",
    "def process_video_for_emotions(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    ground_truth = {}\n",
    "    frame_interval = 5  # Process every 5th frame\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_count += 1\n",
    "        if frame_count % frame_interval != 0:\n",
    "            continue\n",
    "        temp_frame_path = os.path.join(UPLOAD_DIR, f\"frame_{frame_count}.jpg\")\n",
    "        cv2.imwrite(temp_frame_path, frame)\n",
    "        predicted_emotion = detect_emotion(temp_frame_path)\n",
    "        if predicted_emotion:\n",
    "            ground_truth[frame_count] = [predicted_emotion]\n",
    "        os.remove(temp_frame_path)\n",
    "    cap.release()\n",
    "    return ground_truth\n",
    "\n",
    "def evaluate_model(ground_truth, predictions):\n",
    "    flat_predictions = [label for frame_preds in predictions.values() for label in frame_preds]\n",
    "    flat_true_labels = [label for frame_labels in ground_truth.values() for label in frame_labels]\n",
    "    min_length = min(len(flat_true_labels), len(flat_predictions))\n",
    "    flat_true_labels = flat_true_labels[:min_length]\n",
    "    \n",
    "    flat_predictions = flat_predictions[:min_length]\n",
    "    precision = precision_score(flat_true_labels, flat_predictions, average='macro')\n",
    "    recall = recall_score(flat_true_labels, flat_predictions, average='macro')\n",
    "    f1 = f1_score(flat_true_labels, flat_predictions, average='macro')\n",
    "    logging.info(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-score: {f1:.2f}\")\n",
    "\n",
    "def gen_frames(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Resize for consistency\n",
    "        frame = cv2.resize(frame, (640, 480))\n",
    "\n",
    "        # Detect faces + classify emotions\n",
    "        boxes, confidences = detect_faces(frame, face_model)\n",
    "        for box, confidence in zip(boxes, confidences):\n",
    "            if confidence > 0.6:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                face_roi = frame[y1:y2, x1:x2]\n",
    "                if face_roi.size == 0:\n",
    "                    continue\n",
    "                predicted_emotion = classify_emotion(face_roi, emotion_model)\n",
    "                cv2.putText(\n",
    "                    frame, predicted_emotion, (x1, max(20, y1 - 10)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2\n",
    "                )\n",
    "\n",
    "        # Encode frame as JPEG\n",
    "        _, buffer = cv2.imencode('.jpg', frame)\n",
    "        frame_bytes = buffer.tobytes()\n",
    "\n",
    "        # Yield frame for MJPEG\n",
    "        yield (b'--frame\\r\\n'\n",
    "               b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame_bytes + b'\\r\\n')\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "\n",
    "@app.get(\"/webcam_feed\")\n",
    "async def webcam_feed(request: Request):\n",
    "    async def generate_webcam():\n",
    "        cap = cv2.VideoCapture(0)  # 0 = default webcam\n",
    "        try:\n",
    "            while True:\n",
    "                # Stop if the client disconnected (e.g. user clicked \"Stop Webcam\")\n",
    "                if await request.is_disconnected():\n",
    "                    break\n",
    "\n",
    "                success, frame = cap.read()\n",
    "                if not success:\n",
    "                    break\n",
    "\n",
    "                # Face detection + emotion classification\n",
    "                boxes, confidences = detect_faces(frame, face_model)\n",
    "                for box, confidence in zip(boxes, confidences):\n",
    "                    if confidence > 0.6:\n",
    "                        x1, y1, x2, y2 = map(int, box)\n",
    "                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                        face_roi = frame[y1:y2, x1:x2]\n",
    "                        if face_roi.size > 0:\n",
    "                            predicted_emotion = classify_emotion(face_roi, emotion_model)\n",
    "                            cv2.putText(frame, predicted_emotion, (x1, max(20, y1 - 10)),\n",
    "                                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "                # Encode frame as JPEG for MJPEG streaming\n",
    "                _, buffer = cv2.imencode('.jpg', frame)\n",
    "                frame_bytes = buffer.tobytes()\n",
    "                yield (b'--frame\\r\\n'\n",
    "                       b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame_bytes + b'\\r\\n')\n",
    "        finally:\n",
    "            cap.release()\n",
    "            logging.info(\"✅ Webcam released after client disconnected.\")\n",
    "\n",
    "    return StreamingResponse(generate_webcam(),\n",
    "                             media_type=\"multipart/x-mixed-replace; boundary=frame\")\n",
    "\n",
    "\n",
    "# Streaming endpoint\n",
    "@app.get(\"/video_feed\")\n",
    "async def video_feed(file: str):\n",
    "    video_path = os.path.join(UPLOAD_DIR, file)\n",
    "    if not os.path.exists(video_path):\n",
    "        return {\"error\": \"Video not found\"}\n",
    "    return StreamingResponse(gen_frames(video_path),\n",
    "                             media_type=\"multipart/x-mixed-replace; boundary=frame\")\n",
    "\n",
    "@app.post(\"/process_video\")\n",
    "async def process_video(file: UploadFile = File(...)):\n",
    "    # Save uploaded video\n",
    "    file_path = os.path.join(UPLOAD_DIR, file.filename)\n",
    "    with open(file_path, \"wb\") as buffer:\n",
    "        shutil.copyfileobj(file.file, buffer)\n",
    "\n",
    "    logging.info(f\"Video uploaded: {file.filename}\")\n",
    "\n",
    "    # Instead of saving processed video, just return a stream URL\n",
    "    return {\n",
    "        \"filename\": file.filename,\n",
    "        \"stream_url\": f\"/video_feed?file={file.filename}\"\n",
    "    }\n",
    "\n",
    "\n",
    "@app.get(\"/uploads/{filename}\")\n",
    "async def stream_full_video(filename: str):\n",
    "    video_path = os.path.join(UPLOAD_DIR, filename)\n",
    "    if not os.path.exists(video_path):\n",
    "        return {\"error\": \"Video not found\"}\n",
    "    return FileResponse(video_path, media_type=\"video/mp4\")\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def get_html():\n",
    "    with open(\"index.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "        return HTMLResponse(content=f.read(), status_code=200)\n",
    "    \n",
    "@app.post(\"/upload\")\n",
    "async def upload_file(file: UploadFile = File(...)):\n",
    "    file_path = os.path.join(UPLOAD_DIR, file.filename)\n",
    "    with open(file_path, \"wb\") as buffer:\n",
    "        buffer.write(await file.read())\n",
    "    \n",
    "    return JSONResponse({\"url\": f\"/uploads/{file.filename}\"})    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = uvicorn.Config(app, host=\"127.0.0.1\", port=8000, reload=False, log_level=\"info\")\n",
    "server = uvicorn.Server(config)\n",
    "try:\n",
    "    server_task.cancel()\n",
    "except NameError:\n",
    "    pass\n",
    "except Exception as e:\n",
    "    print(\"No previous server task to cancel:\", e)\n",
    "\n",
    "# Start new server in background\n",
    "server_task = asyncio.create_task(server.serve())\n",
    "print(\"🚀 Server started at http://127.0.0.1:8000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    server_task.cancel()\n",
    "    print(\"🛑 Server stopped\")\n",
    "except NameError:\n",
    "    print(\"⚠️ No server task found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
